{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b9c46b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Looking at the Kaggle dataset [Realistic Loan Approval Dataset | US & Canada](https://www.kaggle.com/datasets/parthpatel2130/realistic-loan-approval-dataset-us-and-canada/data) for its major features, and preparing it for classification.\n",
    "\n",
    "\n",
    "## Description of dataset from Author\n",
    "\n",
    "1️⃣ Real-World Approval Logic The dataset implements actual banking criteria:\n",
    "\n",
    " - DTI ratio > 50% = automatic rejection\n",
    " - Defaults on file = instant reject\n",
    " - Credit score bands match real lending thresholds\n",
    " - Employment verification for loans ≥$20K\n",
    "\n",
    "\n",
    "2️⃣ Realistic Correlations\n",
    "\n",
    " - Higher income → Better credit scores\n",
    " - Older applicants → Longer credit history\n",
    " - Students → Lower income, special treatment for small loans\n",
    " - Loan intent affects approval (Education best, Debt Consolidation worst)\n",
    "\n",
    "\n",
    "3️⃣ Product-Specific Rules\n",
    "\n",
    " - Cards: More lenient, higher limits\n",
    " - Personal Loans: Standard criteria, up to $100K\n",
    " - Line of Credit: Capped at $50K, manual review for high amounts\n",
    "\n",
    "\n",
    "4️⃣ Edge Cases Included\n",
    "\n",
    " - Young applicants (age 18) building first credit\n",
    " - Students with thin credit files\n",
    " - Self-employed with variable income\n",
    " - High debt-to-income ratios\n",
    " - Multiple delinquencies\n",
    "\n",
    "\n",
    "## Exploration Steps\n",
    "\n",
    "1. Fetch Data and Store Artifact\n",
    "2. Initial Visual and Tabular Assessment\n",
    "3. Data Cleaning\n",
    "4. Model Training and Selection\n",
    "5. Model Assessment Against Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1daff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3 Standard Library\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# if you are re-running this on your system, you'll probably need to change this path to match where you are storing these files\n",
    "PROJECT_ROOT_PATH = Path(f\"{os.environ['USERPROFILE']}\\\\OneDrive\\\\Education\\\\WGU\\\\Capstone\")\n",
    "\n",
    "# making sure I'm in the right directory for EDA, need to be in root\n",
    "if not re.match(r'.*Capstone$', os.getcwd()):\n",
    "    os.chdir(PROJECT_ROOT_PATH)\n",
    "\n",
    "# Data Science Modules\n",
    "## Data Analytics and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "## Machine Learning\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate, train_test_split\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, precision_score, fbeta_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Custom modules\n",
    "from src.utilities import new_logger, save_atomic\n",
    "\n",
    "# Setting Pandas DataFrame options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d5407",
   "metadata": {},
   "source": [
    "This is a module-wide logger that is tied to this notebook. During operation, it may also trigger writes to `logs/utils/src.utilities.log` when using `src.utilities` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3e7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = new_logger(\"eda.data_exploration\", \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0a2dd",
   "metadata": {},
   "source": [
    "### 1. Fetch Data and Store Artifact\n",
    "\n",
    "This dataset was downloaded locally as a CSV file.\n",
    "\n",
    "I'm being careful to use `Path` in order to create absolute references to files on disk. This will help prevent odd behavior for relative references when run on other operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d73e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch local dataset\n",
    "data_path = Path('data/Loan_approval_data_2025.csv').resolve()\n",
    "logger.info(f\"Attempting to fetch data from '{data_path}'\")\n",
    "\n",
    "loan_appr_df = pd.read_csv(data_path)\n",
    "logger.info(f\"Successfully created DataFrame {loan_appr_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c93740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving original in Parquet format for submission as artifact\n",
    "orig_data_path_parquet = save_atomic(loan_appr_df, Path(\"data/loan_approval_data_2025.orig.parquet\"), fmt=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46eca19",
   "metadata": {},
   "source": [
    "### 2. Initial Visual and Tabular Assessment\n",
    "\n",
    "#### Tabular Assessment (Directed and Non-Directed)\n",
    "\n",
    "During this part of the process I am looking for multiple issues common to datasets from the Internet:\n",
    "- Missing data\n",
    "- Malformed data/typos\n",
    "- Improper data ranges\n",
    "- Improper data types\n",
    "\n",
    "The first section deals with looking for missing data and dtypes, then we move on to looking at the correlations between numeric features as well as their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703dc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_appr_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f1c1d",
   "metadata": {},
   "source": [
    "There doesn't seem to be any missing data. This makes sense because this is a synthetic dataset, though it is based on realistic business rules surrounding approving or denying loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_appr_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4de80c",
   "metadata": {},
   "source": [
    "Now I'll look for duplicated rows within the dataset. Since this is synthetically generated data, I don't expect to see any duplicated rows. This will exclude the `customer_id` column since that column will make each row unique by default.\n",
    "\n",
    "A value of 0 indicates that there are not duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_appr_df.drop(columns=['customer_id']).duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a21097",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_appr_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035ca66",
   "metadata": {},
   "source": [
    "The basic descriptive statistics for each numeric columns look reasonable based on my knowledge of each of these columns.\n",
    "\n",
    "Now I'll do a non-directed assessment of the head, tail, and a random sample of rows within the dataset. This is an attempt to look for consistency throughout the data. I am also looking for situations where the columns should be joined, melted, or otherwise engineered to produce a better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e099f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_appr_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_appr_df.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_appr_df.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5c363",
   "metadata": {},
   "source": [
    "#### Exploratory Visualizations\n",
    "\n",
    "First, I isolate the columns into their various types. Since there are so few `object` columns, I will create that column grouping first, and then use it to narrow down to the numeric types. For ease of analysis, I will make two separate numeric groupings - one with the target variable `loan_status` and one without.\n",
    "\n",
    "Then, the `obj_cols` group will be examined for repeated values - this indicate a good candidate for a categorical column.\n",
    "\n",
    "Next, the `num_cols` will be analyzed using a correlation heatmap to look at relationships among variables. [I may also look at various slices of the data to see how relationships may change??]\n",
    "\n",
    "Finally, I will take a look at the distributions of the `num_cols` group to get a sense of how normal each distribution is, or whether it has unique characteristics such as discrete values, or bimodality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e789b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns, currently represented as `object` (string) dtypes\n",
    "obj_cols = [col for col in loan_appr_df.dtypes[loan_appr_df.dtypes == 'object'].index if col != 'customer_id']\n",
    "\n",
    "# all numerical columns including the `loan_status` target variable\n",
    "num_cols_with_target = loan_appr_df.drop(columns=obj_cols).drop(columns=['customer_id']).columns.values\n",
    "\n",
    "# all numerical columns excluding the `loan_status` target variable\n",
    "num_cols = loan_appr_df.drop(columns=obj_cols).drop(columns=['customer_id', 'loan_status']).columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f734aaf",
   "metadata": {},
   "source": [
    "#### Categorical Feature Exploration\n",
    "\n",
    "Moving on to the object features, looking at the current possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in obj_cols:\n",
    "    print(f\"{loan_appr_df[col].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20a8c1",
   "metadata": {},
   "source": [
    "Based on what we can see above, these columns are all good candidates for becoming categorical. Let's look at the distributions of each feature to see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in obj_cols:\n",
    "    val_counts = loan_appr_df[col].value_counts()\n",
    "    fig, ax = plt.subplots(figsize=(15,9))\n",
    "    ax.bar(val_counts.index.values, val_counts.values)\n",
    "    ax.set_xlabel(val_counts.index.name)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(f\"Frequency of {val_counts.index.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b40fa9",
   "metadata": {},
   "source": [
    "These categories seem like they make sense, and no changes are needed. This is expected from a synthetic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00b94a",
   "metadata": {},
   "source": [
    "#### Numerical Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix with all of the numeric values\n",
    "correlation_matrix = loan_appr_df[num_cols_with_target].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Seaborn heatmap showing all of the correlation values\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"vlag\", fmt=\".2f\")\n",
    "plt.title(\"Loan Application: Numeric Variables Correlation Heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d8c23",
   "metadata": {},
   "source": [
    "Moving on, we can summarize how each numeric variable varies against every other numeric variable using Seaborn's `pairplot()` function. I will take a look at each distribution individually as well, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc8cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(loan_appr_df[num_cols_with_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ba94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    fig, ax = plt.subplots(figsize=(15,9))\n",
    "    n, bins, patches = ax.hist(loan_appr_df[col])\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(f\"Distribution of {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d8334",
   "metadata": {},
   "source": [
    "These distributions look appropriate for each category, no major changes will be required. There is no case of \"normal\" distributions, which for looking at human populations is expected. The only \"bimodal\" feature seen is `loan_status` which doesn't really count since it's the binary response variable (the classification we're making).\n",
    "\n",
    "Other non-continuous distributions are for `derogatory_marks`, `defaults_on_file`, and `delinquencies_last_2_years` are expected as these are discrete variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f1151",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning\n",
    "\n",
    "Based on what I can see in this dataset, there is barely any cleaning that must take place. Instead, there simply needs to be a pipeline to create a `OneHotEncoder` for the categorical columns and `StandardScaler` for the numeric columns for use in a Random Forest Classifier.\n",
    "\n",
    "At this point, just dropping the customer ID column because it isn't useful in classification. This may have been able to be done earlier in the process as well. An alternative treatment is to use this column as the index, rather than the existing non-semantic index.\n",
    "\n",
    "In addition, I will adjust the dtype of the object columns to be categorical for more efficient storage of data moving forward, and to solidify the idea that they are categories that will benefit from One-Hot Encoding before training the various models.\n",
    "\n",
    "Finally, I have removed the `payment_to_income_ratio` column because it has perfect correlation with `loan_to_income_ratio` and does not add any value to this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03da7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unique Customer_ID identifier\n",
    "loan_appr_wip = loan_appr_df.drop(columns=['customer_id', 'payment_to_income_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8691a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make each object feature a category feature instead\n",
    "for col in obj_cols:\n",
    "    loan_appr_wip[col] = loan_appr_wip[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f5c860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean dataset as Parquet, for logging to W&B later\n",
    "data_path = save_atomic(loan_appr_wip, Path(\"data/loan_approval_data_2025.clean.parquet\"), fmt=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaacdbb",
   "metadata": {},
   "source": [
    "Now that cleaning is complete, let's perform a quick look at the clean DataFrame to make sure that the dtypes are correct and we still have no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fad34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                   Non-Null Count  Dtype   \n",
      "---  ------                   --------------  -----   \n",
      " 0   age                      50000 non-null  int64   \n",
      " 1   occupation_status        50000 non-null  category\n",
      " 2   years_employed           50000 non-null  float64 \n",
      " 3   annual_income            50000 non-null  int64   \n",
      " 4   credit_score             50000 non-null  int64   \n",
      " 5   credit_history_years     50000 non-null  float64 \n",
      " 6   savings_assets           50000 non-null  int64   \n",
      " 7   current_debt             50000 non-null  int64   \n",
      " 8   defaults_on_file         50000 non-null  int64   \n",
      " 9   delinquencies_last_2yrs  50000 non-null  int64   \n",
      " 10  derogatory_marks         50000 non-null  int64   \n",
      " 11  product_type             50000 non-null  category\n",
      " 12  loan_intent              50000 non-null  category\n",
      " 13  loan_amount              50000 non-null  int64   \n",
      " 14  interest_rate            50000 non-null  float64 \n",
      " 15  debt_to_income_ratio     50000 non-null  float64 \n",
      " 16  loan_to_income_ratio     50000 non-null  float64 \n",
      " 17  loan_status              50000 non-null  int64   \n",
      "dtypes: category(3), float64(5), int64(10)\n",
      "memory usage: 5.9 MB\n"
     ]
    }
   ],
   "source": [
    "loan_appr_wip.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb0c6a",
   "metadata": {},
   "source": [
    "Now I have to redefine the various column lists, since the WIP DataFrame is not the same as the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab92227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns, currently represented as `object` (string) dtypes\n",
    "obj_cols = [col for col in loan_appr_wip.dtypes[loan_appr_wip.dtypes == 'category'].index]\n",
    "\n",
    "# all numerical columns including the `loan_status` target variable\n",
    "num_cols_with_target = loan_appr_wip.drop(columns=obj_cols).columns.values\n",
    "\n",
    "# all numerical columns excluding the `loan_status` target variable\n",
    "num_cols = loan_appr_wip.drop(columns=obj_cols).drop(columns=['loan_status']).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eadb7ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['occupation_status', 'product_type', 'loan_intent']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6dd41",
   "metadata": {},
   "source": [
    "### 4. Model Training and Selection\n",
    "\n",
    "This section of the notebook provides the basis for the model training and selection process that will be implemented as a part of the completed MLOps pipeline.\n",
    "\n",
    "First, I will split the X and y variables. Then, I will generate the training and testing sets. One alternative that may produce a better model is to perform cross-validation, switching up the training and testing sets to take full advantage of all of our available data. TODO: look into implementing cross-validation in this case, how would I treat the metrics and the model in that case?\n",
    "\n",
    "Next, I will create a machine learning pipeline that includes the preprocessing steps for both the categorical (`OneHotEncoder`) and numeric (`StandardScaler`) variables. This ensures that the trained pipeline can be used for both training and inference as the same steps are being performed in both instances (provided the DataFrame columns are similar).\n",
    "\n",
    "Finally, that machine learning pipeline will be trained using the `GridSearchCV` method for hyperparameter tuning. This will allow me to narrow in on which parameters are best for this scenario to reach the optimal hyperparameters for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ffbcbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into X and y sets\n",
    "y = loan_appr_wip.pop('loan_status')\n",
    "X = loan_appr_wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddfab451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 17 columns):\n",
      " #   Column                   Non-Null Count  Dtype   \n",
      "---  ------                   --------------  -----   \n",
      " 0   age                      50000 non-null  int64   \n",
      " 1   occupation_status        50000 non-null  category\n",
      " 2   years_employed           50000 non-null  float64 \n",
      " 3   annual_income            50000 non-null  int64   \n",
      " 4   credit_score             50000 non-null  int64   \n",
      " 5   credit_history_years     50000 non-null  float64 \n",
      " 6   savings_assets           50000 non-null  int64   \n",
      " 7   current_debt             50000 non-null  int64   \n",
      " 8   defaults_on_file         50000 non-null  int64   \n",
      " 9   delinquencies_last_2yrs  50000 non-null  int64   \n",
      " 10  derogatory_marks         50000 non-null  int64   \n",
      " 11  product_type             50000 non-null  category\n",
      " 12  loan_intent              50000 non-null  category\n",
      " 13  loan_amount              50000 non-null  int64   \n",
      " 14  interest_rate            50000 non-null  float64 \n",
      " 15  debt_to_income_ratio     50000 non-null  float64 \n",
      " 16  loan_to_income_ratio     50000 non-null  float64 \n",
      "dtypes: category(3), float64(5), int64(9)\n",
      "memory usage: 5.5 MB\n"
     ]
    }
   ],
   "source": [
    "X.shape\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d6a71f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b27efd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a column transformer out of OneHotEncoder and StandardScaler\n",
    "logger.info(\"Starting inference training pipeline for all models\")\n",
    "cat_preproc = OneHotEncoder()\n",
    "logger.debug(f\"Created OneHotEncoder for columns {obj_cols}\")\n",
    "num_preproc = StandardScaler()\n",
    "logger.debug(f\"Created StandardScaler for columns {num_cols}\")\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_transform\", cat_preproc, obj_cols),\n",
    "        (\"num_transform\", num_preproc, num_cols)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    verbose=True\n",
    ")\n",
    "logger.debug(\"Created ColumnTransformer for categorical and numerical preprocessing with all above columns. Any other columns will be dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c6c0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;cat_transform&#x27;, OneHotEncoder(),\n",
       "                                 [&#x27;occupation_status&#x27;, &#x27;product_type&#x27;,\n",
       "                                  &#x27;loan_intent&#x27;]),\n",
       "                                (&#x27;num_transform&#x27;, StandardScaler(),\n",
       "                                 array([&#x27;age&#x27;, &#x27;years_employed&#x27;, &#x27;annual_income&#x27;, &#x27;credit_score&#x27;,\n",
       "       &#x27;credit_history_years&#x27;, &#x27;savings_assets&#x27;, &#x27;current_debt&#x27;,\n",
       "       &#x27;defaults_on_file&#x27;, &#x27;delinquencies_last_2yrs&#x27;, &#x27;derogatory_marks&#x27;,\n",
       "       &#x27;loan_amount&#x27;, &#x27;interest_rate&#x27;, &#x27;debt_to_income_ratio&#x27;,\n",
       "       &#x27;loan_to_income_ratio&#x27;], dtype=object))],\n",
       "                  verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for ColumnTransformer</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></div></label><div class=\"sk-toggleable__content \" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transformers&nbsp;</td>\n",
       "            <td class=\"value\">[(&#x27;cat_transform&#x27;, ...), (&#x27;num_transform&#x27;, ...)]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('remainder',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">remainder&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;drop&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sparse_threshold&nbsp;</td>\n",
       "            <td class=\"value\">0.3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformer_weights',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transformer_weights&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose_feature_names_out',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose_feature_names_out&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('force_int_remainder_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">force_int_remainder_cols&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>cat_transform</div></div></label><div class=\"sk-toggleable__content \" data-param-prefix=\"cat_transform__\"><pre>[&#x27;occupation_status&#x27;, &#x27;product_type&#x27;, &#x27;loan_intent&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content \" data-param-prefix=\"cat_transform__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">categories&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('drop',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">drop&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_output',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sparse_output&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dtype&nbsp;</td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.float64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('handle_unknown',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">handle_unknown&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;error&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_frequency',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_frequency&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_categories&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_name_combiner',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">feature_name_combiner&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;concat&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>num_transform</div></div></label><div class=\"sk-toggleable__content \" data-param-prefix=\"num_transform__\"><pre>[&#x27;age&#x27; &#x27;years_employed&#x27; &#x27;annual_income&#x27; &#x27;credit_score&#x27;\n",
       " &#x27;credit_history_years&#x27; &#x27;savings_assets&#x27; &#x27;current_debt&#x27; &#x27;defaults_on_file&#x27;\n",
       " &#x27;delinquencies_last_2yrs&#x27; &#x27;derogatory_marks&#x27; &#x27;loan_amount&#x27;\n",
       " &#x27;interest_rate&#x27; &#x27;debt_to_income_ratio&#x27; &#x27;loan_to_income_ratio&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content \" data-param-prefix=\"num_transform__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_mean',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_mean&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_std',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_std&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('cat_transform', OneHotEncoder(),\n",
       "                                 ['occupation_status', 'product_type',\n",
       "                                  'loan_intent']),\n",
       "                                ('num_transform', StandardScaler(),\n",
       "                                 array(['age', 'years_employed', 'annual_income', 'credit_score',\n",
       "       'credit_history_years', 'savings_assets', 'current_debt',\n",
       "       'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks',\n",
       "       'loan_amount', 'interest_rate', 'debt_to_income_ratio',\n",
       "       'loan_to_income_ratio'], dtype=object))],\n",
       "                  verbose=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b2c9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation(pipeline: Pipeline, X: pd.DataFrame, y: pd.Series, est_name: str, param_grid: dict[str, list], trials: int = 1, outer_cv_splits: int = 5, inner_cv_splits: int = 3, random_state: int = 72925, verbose: int = 0, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    \"\"\"Using a Machine Learning Pipeline and Parameter Grid, perform nested Cross-Validation and return a DataFrame with the results.\n",
    "\n",
    "    Please not that this function can take a LONG time to run, if the hyperparameter space is large, or if the combined number of cross-validation folds and trials is very large.\n",
    "\n",
    "    This function has a very high time-complexity, and should only be run if you have a lot of time to spare. To put this in common terms, if we have four hyperparameters with 3 options each, we have a hyperparameter space of \n",
    "\n",
    "    Args:\n",
    "        pipeline (sklearn.pipeline.Pipeline):\n",
    "            The machine learning Pipeline that contains the preprocessed columns and the model to use.\n",
    "        X (pd.DataFrame):\n",
    "            The X matrix to use for training and testing, contains only the predictor variables.\n",
    "        y (pd.Series):\n",
    "            the y array to use for training and testing, contains only the reponse variable.\n",
    "        est_name (str):\n",
    "            The estimator name, for logging.\n",
    "        param_grid (dict[str, list]):\n",
    "            A parameter grid dictionary with the parameter names as 'model__parameter' as keys and the list of hyperparameter options as the values.\n",
    "        trials (int):\n",
    "            The number of cross-validation trials to perform, defaults to 1.\n",
    "        outer_cv_splits (int):\n",
    "            The number of cross-validation splits for each trial, defaults to 5.\n",
    "        inner_cv_splits (int):\n",
    "            The number of hyperparameter tuning splits for each outer cross-validation fold, defaults to 3.\n",
    "        random_state (int):\n",
    "            The random state to use for better comparison across models. Defaults to 72925.\n",
    "        verbose (int):\n",
    "            The level of verbosity to use for the GridSearchCV and cross_validate methods, defaults to 0, letting the loops show the progress.\n",
    "        n_jobs (int):\n",
    "            The number of processors to use to parallelize the jobs. Defaults to -1.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: a results DataFrame with the fit_time, score_time, estimator object, F-Beta, Accuracy, Precision, ROC_AUC, and the best hyperparameters for that model.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting nested cross-validation for {est_name}\")\n",
    "    logger.info(f\"Using parameter grid for GridSearchCV: {param_grid}\")\n",
    "\n",
    "    nested_model_results = []\n",
    "\n",
    "    # inner tqdm loop showing trials, from Walters 2022\n",
    "    for _ in tqdm(range(trials), desc=f\"{est_name} Cross Validation Trials\", leave=True):\n",
    "        # define Inner CV Loop\n",
    "        inner_cv = KFold(n_splits=inner_cv_splits, shuffle=True, random_state=random_state)\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, verbose=verbose, cv=inner_cv, n_jobs=n_jobs)\n",
    "\n",
    "        # define Outer CV Loop\n",
    "        outer_cv = KFold(n_splits=outer_cv_splits, shuffle=True, random_state=random_state)\n",
    "        nested_model_results.append(\n",
    "            cross_validate(\n",
    "                grid_search,\n",
    "                X,\n",
    "                y,\n",
    "                cv=outer_cv,\n",
    "                scoring={\n",
    "                    \"fbeta\": make_scorer(fbeta_score, beta=0.5),\n",
    "                    \"accuracy\": make_scorer(accuracy_score),\n",
    "                    \"precision\": make_scorer(precision_score),\n",
    "                    \"roc_auc\": make_scorer(roc_auc_score)\n",
    "                },\n",
    "                return_estimator=True,\n",
    "                verbose=verbose,\n",
    "                n_jobs=n_jobs\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # place results in a DataFrame\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for result in nested_model_results:\n",
    "        if results.shape[0] != 0:\n",
    "            # concat to existing DataFrame\n",
    "            results = pd.concat([results, pd.DataFrame(result)])\n",
    "        else:\n",
    "            # create DataFrame\n",
    "            results = pd.DataFrame(result)\n",
    "\n",
    "    # add the hyperparameters to the results DataFrame\n",
    "    results['hyperparameters'] = [est.best_params_ for est in results['estimator']]\n",
    "    # name the model\n",
    "    results['model'] = est_name\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ebe909",
   "metadata": {},
   "source": [
    "To actually perform the training and tuning, run the following code with the model dictionary as follows. This contains all of the model names, their objects, and their hyperparameter grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69c85786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definitions = [\n",
    "    {\n",
    "        \"name\": \"Support Vector Machine\",\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('preprocessing', preproc), # Preprocessing step\n",
    "            ('clf', SVC(gamma='scale', max_iter=-1,\n",
    "                        random_state=72925)) # Model step\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            \"clf__C\": list(np.logspace(-4,4,4)),\n",
    "            \"clf__kernel\": ['rbf', 'poly'],\n",
    "            \"clf__degree\": [3,4,5] # for 'poly' kernel only\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('preprocessing', preproc), # Preprocessing step\n",
    "            ('clf', LogisticRegression(penalty='l2', random_state=72925)) # Model step\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            # C must be positive, starting with default value and moving up on log scale 4 places\n",
    "            \"clf__C\": list(np.logspace(-4,4,4)),\n",
    "            \"clf__solver\": ['lbfgs', 'sag', 'saga', 'newton-cholesky'],\n",
    "            \"clf__max_iter\": [x for x in range(1000,2001,250)]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gaussian Naive Bayes\",\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('preprocessing', preproc), # Preprocessing step\n",
    "            ('clf', GaussianNB()) # Model step\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            \"clf__var_smoothing\": list(np.logspace(0,-9, num=100))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Adaptive Boosting\",\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('preprocessing', preproc), # Preprocessing step\n",
    "            ('clf', AdaBoostClassifier(random_state=72925)) # Model step\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            \"clf__n_estimators\": [x for x in range(50,250,50)],\n",
    "            \"clf__learning_rate\": [10**x for x in [-2,-1,0]]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('preprocessing', preproc), # Preprocessing step\n",
    "            ('clf', RandomForestClassifier()) # Model step\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            'clf__n_estimators': [10**x for x in range(0,4)],\n",
    "            'clf__max_features': ['sqrt'],\n",
    "            'clf__max_depth': [x for x in range(1,6)],\n",
    "            'clf__min_samples_split': [x*2 for x in range(1,6)]\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "842ed429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be73d379ecf9497fa44f5baf516a1164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Training Experiment Loop:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5482ac4821a84de8ac91203fbaf25d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Support Vector Machine Cross Validation Trials:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 14 concurrent workers.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 1450] Insufficient system resources exist to complete the requested service",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m tqdm(model_definitions, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel Training Experiment Loop\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     13\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining iteration with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as the inference model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     _result = \u001b[43mnested_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpipeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparam_grid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults DataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is complete with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_result.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# log the specific _result DataFrame to disk for recovery purposes, this is a very long training process and we don't want to overwrite if we can help it\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mnested_cross_validation\u001b[39m\u001b[34m(pipeline, X, y, est_name, param_grid, trials, outer_cv_splits, inner_cv_splits, random_state, verbose, n_jobs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# define Outer CV Loop\u001b[39;00m\n\u001b[32m     47\u001b[39m     outer_cv = KFold(n_splits=outer_cv_splits, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=random_state)\n\u001b[32m     48\u001b[39m     nested_model_results.append(\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgrid_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mouter_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfbeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfbeta_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccuracy_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprecision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroc_auc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroc_auc_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     )\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# place results in a DataFrame\u001b[39;00m\n\u001b[32m     67\u001b[39m results = pd.DataFrame()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dalla\\OneDrive\\Education\\WGU\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dalla\\OneDrive\\Education\\WGU\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:399\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dalla\\OneDrive\\Education\\WGU\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dalla\\OneDrive\\Education\\WGU\\Capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dalla\\OneDrive\\Education\\WGU\\Capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dalla\\OneDrive\\Education\\WGU\\Capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1450] Insufficient system resources exist to complete the requested service"
     ]
    }
   ],
   "source": [
    "temp_results_path = Path('data/model_metrics.wip.parquet')\n",
    "final_results_path = Path('data/model_metrics.final.parquet')\n",
    "\n",
    "if not final_results_path.exists():\n",
    "    if not temp_results_path.exists():\n",
    "        # no file found on disk, create an empty DataFrame to start with\n",
    "        all_results = pd.DataFrame()\n",
    "    else:\n",
    "        # load the existing DataFrame and add to it without overwriting initially\n",
    "        all_results = pd.read_parquet(temp_results_path)\n",
    "\n",
    "    for model in tqdm(model_definitions, desc=f\"Model Training Experiment Loop\", leave=True):\n",
    "        logger.debug(f\"Training iteration with {model['name']} as the inference model.\")\n",
    "\n",
    "        _result = nested_cross_validation(model['pipeline'], X, y, model['name'], model['param_grid'], trials=5, verbose=3)\n",
    "        logger.debug(f\"Results DataFrame for {model['name']} is complete with shape {_result.shape}\")\n",
    "\n",
    "        # log the specific _result DataFrame to disk for recovery purposes, this is a very long training process and we don't want to overwrite if we can help it\n",
    "        _model_save_path = Path(f\"data/model_metrics_{model['name'].lower().replace(' ', '-')}.wip.parquet\")\n",
    "        logger.info(f\"Saving the results from the {model['name']} training runs to {_model_save_path} for safe-keeping.\")\n",
    "        save_atomic(_result.drop(columns=['estimator']), _model_save_path)\n",
    "\n",
    "        if all_results.shape[0] == 0:\n",
    "            # empty DataFrame\n",
    "            all_results = _result\n",
    "            logger.debug(f\"Initialized all_results DataFrame from the {model['name']} _result DataFrame.\")\n",
    "        else:\n",
    "            # adding to DataFrame instead\n",
    "            all_results = pd.concat([all_results, _result])\n",
    "            logger.debug(f\"Added the {model['name']} _result DataFrame to the existing all_results DataFrame. Current shape: {all_results.shape}\")\n",
    "        \n",
    "        # log DataFrame to disk so that it can be recovered if \n",
    "        logger.info(f\"Saving the results from all currently completed training runs to {temp_results_path} for safe-keeping.\")\n",
    "        save_atomic(all_results.drop(columns=['estimator']), temp_results_path)\n",
    "\n",
    "    # if you reach this point, CONGRATS you are done, thanks for playing!\n",
    "    logger.info(f\"Saving the final results of training to {final_results_path}.\")\n",
    "    save_atomic(all_results.drop(columns=['estimator']), final_results_path)\n",
    "else:\n",
    "    # final results have been created, load into DataFrame and don't repeat the process\n",
    "    all_results = pd.read_parquet(final_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.groupby(by=['model']).agg({\n",
    "    'fit_time': 'mean',\n",
    "    'score_time': 'mean',\n",
    "    'test_fbeta': 'mean',\n",
    "    'test_accuracy': 'mean',\n",
    "    'test_precision': 'mean', \n",
    "    'test_roc_auc': 'mean'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f98d77",
   "metadata": {},
   "source": [
    "These code blocks are for each individual model, able to be trained separately from the rest of the models as needed. They have been commented out to allow the core training loop to run as normal.\n",
    "\n",
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_pipeline = Pipeline([\n",
    "#     ('preprocessing', preproc), # Preprocessing step\n",
    "#     ('clf', RandomForestClassifier()) # Model step\n",
    "# ])\n",
    "# logger.debug(\"Pipeline created with RandomForestClassifier as the inference model.\")\n",
    "\n",
    "# # Define the parameter grid for hyperparameter tuning\n",
    "# rf_param_grid = {\n",
    "#     'clf__n_estimators': [10**x for x in range(0,4)],\n",
    "#     'clf__max_features': [0.3, 0.5, 'sqrt'],\n",
    "#     'clf__max_depth': [x for x in range(1,6)],\n",
    "#     'clf__min_samples_split': [x*2 for x in range(1,6)]\n",
    "# }\n",
    "\n",
    "# rf_results = nested_cross_validation(rf_pipeline, X, y, \"Random Forest\", rf_param_grid, trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0025a",
   "metadata": {},
   "source": [
    "In the interest of exploring how multiple classification models perform against one another, let's work on training multiple classifiers and comparing them against one another.\n",
    "\n",
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ceb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train a logistic regression model on this same data, with hyperparameter tuning\n",
    "# lr_pipeline = Pipeline([\n",
    "#     ('preprocessing', preproc), # Preprocessing step\n",
    "#     ('clf', LogisticRegression(penalty='l2', random_state=72925)) # Model step\n",
    "# ])\n",
    "# logger.debug(\"Pipeline created with LogisticRegression as the inference model.\")\n",
    "\n",
    "# lr_param_grid = {\n",
    "#     # C must be positive, starting with default value and moving up on log scale 4 places\n",
    "#     \"clf__C\": list(np.logspace(-4,4,4)),\n",
    "#     \"clf__solver\": ['lbfgs', 'liblinear', 'sag', 'saga', 'newton-cholesky'],\n",
    "#     \"clf__max_iter\": [x for x in range(1000,2001,250)]\n",
    "# }\n",
    "# lr_results = nested_cross_validation(lr_pipeline, X, y, \"Logistic Regression\", lr_param_grid, trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a6b62",
   "metadata": {},
   "source": [
    "**AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf88f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train an AdaBoost model on this same data, with hyperparameter tuning\n",
    "# ab_pipeline = Pipeline([\n",
    "#     ('preprocessing', preproc), # Preprocessing step\n",
    "#     ('clf', AdaBoostClassifier(random_state=72925)) # Model step\n",
    "# ])\n",
    "# logger.debug(\"Pipeline created with AdaBoost as the inference model.\")\n",
    "\n",
    "# ab_param_grid = {\n",
    "#     \"clf__n_estimators\": [x for x in range(50,250,50)],\n",
    "#     \"clf__learning_rate\": [10**x for x in [-2,-1,0]],\n",
    "#     \"clf__estimator__max_depth\": [1,2,3]\n",
    "# }\n",
    "\n",
    "# ab_results = nested_cross_validation(ab_pipeline, X, y, \"Adaptive Boosting\", ab_param_grid, trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6079fa",
   "metadata": {},
   "source": [
    "**Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train an SVM model on this same data, with hyperparameter \n",
    "# svc_pipeline = Pipeline([\n",
    "#     ('preprocessing', preproc), # Preprocessing step\n",
    "#     ('clf', SVC(gamma='scale', max_iter=-1,\n",
    "#                 random_state=72925)) # Model step\n",
    "# ])\n",
    "# logger.debug(\"Pipeline created with SVM as the inference model.\")\n",
    "\n",
    "# svc_param_grid = {\n",
    "#     \"clf__C\": list(np.logspace(-4,4,4)),\n",
    "#     \"clf__kernel\": ['rbf', 'poly'],\n",
    "#     \"clf__degree\": [3,4,5,6] # for 'poly' kernel only\n",
    "# }\n",
    "\n",
    "# svc_results = nested_cross_validation(svc_pipeline, X, y, \"Support Vector Machine\", svc_param_grid, trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00118e",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deafb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train a naive Bayes model on this same data, with hyperparameter tuning\n",
    "# be VERY careful about collinearity and assumptions about independence... identify them and show where in the data this is a concern\n",
    "# gnb_pipeline = Pipeline([\n",
    "#     ('preprocessing', preproc), # Preprocessing step\n",
    "#     ('clf', GaussianNB()) # Model step\n",
    "# ])\n",
    "# logger.debug(\"Pipeline created with GaussianNB as the inference model.\")\n",
    "\n",
    "# gnb_param_grid = {\n",
    "#     \"clf__var_smoothing\": list(np.logspace(0,-9, num=100))\n",
    "# }\n",
    "\n",
    "# gnb_results = nested_cross_validation(gnb_pipeline, X, y, \"Gaussian Naive Bayes\", gnb_param_grid, trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3211e24",
   "metadata": {},
   "source": [
    "#### 5. Model Assessment Against Baseline\n",
    "\n",
    "Actually I probably won't compare against a baseline anymore...\n",
    "\n",
    "Now that we've determined which method will work best for our data, we'll attempt to further tune that model and then compare its results against the baseline estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c429220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7292025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb096a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dummy classifier baselines using multiple strategies\n",
    "\n",
    "dummy_strategies = {\n",
    "    \"most_frequent\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"stratified\": DummyClassifier(strategy='stratified', random_state=72925),\n",
    "    \"uniform\": DummyClassifier(strategy=\"uniform\", random_state=72925),\n",
    "    \"constant_0\": DummyClassifier(strategy=\"constant\", constant=0)\n",
    "}\n",
    "\n",
    "# evaluate and fit each dummy strategy\n",
    "results_data = {}\n",
    "\n",
    "for name, clf in dummy_strategies.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_dummy = clf.predict(X_test)\n",
    "    results_data[name] = [\n",
    "        accuracy_score(y_test, y_pred_dummy),\n",
    "        precision_score(y_test, y_pred_dummy, zero_division=0),\n",
    "        fbeta_score(y_test, y_pred_dummy, beta=0.5, zero_division=0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fffef2",
   "metadata": {},
   "source": [
    "### 5. Model Assessment Using McNemar's Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlxtend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone 2 Kernel",
   "language": "python",
   "name": "capstone_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
